\section{Билет 2}

\begin{center}
    \it
    Метод моментов и его состоятельность.
    Метод максимального правдоподобия.
    Энтропия и состоятельность оценки максимального правдоподобия.
\end{center}

\usetikzlibrary{arrows}
\definecolor{BLACK}{rgb}{0,0,1}
\definecolor{BLUE}{rgb}{0.2,0.2,1}

\subsection{Метод моментов и его состоятельность}
Пусть $(X_1, \ldots, X_n)$ --- выборка, где $X_i$ задано распределением $F_{\theta}$. Хотим найти состоятельную оценку параметра $\theta$.
Пусть $g$ --- непрерывная функция, причём $\EE_{\theta} |g(X_1)| < \infty$. Посчитаем матожидание $\EE_{\theta} g(X_1) = f(\theta)$.
Предположим, что $\exists\, f^{-1}\,$, и она непрерывна.
Так как на практике мы не знаем параметр $\theta$, то мы не можем посчитать такое матожидание.
Но мы можем приближённо посчитать $f(\theta)$ воспользовавшись ЗБЧ:
\[
    \frac{g(X_1) + \ldots + g(X_n)}{n} \xrightarrow{P_\theta} \EE_{\theta} g(X_1) = f(\theta).
\]
Теперь в силу обратимости $f$ можно получить сходимость к $\theta$:
\[
    f^{-1}\left( \frac{g(X_1) + \ldots + g(X_n)}{n} \right) \xrightarrow{P_\theta} f^{-1}(f(\theta)).
\]
Оценкой параметра $\theta$ назовём функцию
\begin{equation*}
    \hat{\theta}(X_1, \ldots, X_n) = f^{-1}\left( \frac{g(X_1) + \ldots + g(X_n)}{n} \right)
.\end{equation*}
Эта оценка состоятельна, так как $\hat{\theta}(X_1, \dots, X_n) \xrightarrow{P_\theta} \theta$.
Вместо ЗБЧ можно применить УЗБЧ, и получить сильную состоятельность.

\subsection{Метод максимального правдоподобия}
\subsection{Энтропия и состоятельность оценки максимального правдоподобия}
\begin{definition}
    Обобщённой плотностью $\rho_X$ случайной величины $X$ назовём функцию плотности $X$, если случайная величина является непрерывно, или функцию
    $\rho_X(t) = P(X = t)$ в случае, если $X$ имеет дискретное распределение.
\end{definition}
\begin{definition}
    Пусть $X = (X_1, \ldots, X_n)$ --- выборка из распределения с обобщённой плотностью $\rho_{\theta}$. Обобщённая плотность вектора $X$ называется функцией
    правдоподобия, и имеет вид
    \[
        p(X, \theta) = \rho_{\theta}(X_1) \cdot \ldots \cdot \rho_{\theta}(X_n)
    \]
    Функцию $\ln p(X, \theta)$ называют логарифмической функцией правдоподобия и обозначают $L(X, \theta)$.
\end{definition}
\begin{definition}
    Пусть $\rho_0, \rho_1$ --- положительные вероятностные плотности. Выражение
    \[
        \int \ln\frac{\rho_1(x)}{\rho_0(x)} \cdot \rho_1(x)dx
    \]
    называется энтропией распределения с плотностью $\rho_1$ относительно распределения с плотностью $\rho_0$.
\end{definition}
\begin{comment}
    Здесь и далее интергралы без пределов интегрирования обозначают интегрирование по множеству, на котором задано распределение. Они вовсе не означают
    неопределённый интеграл.
\end{comment}

Следующее утверждение показывает, что энтропия в некотором смысле оценивает расстояние между распределениями:
\begin{lemma*}[Информационное неравенство]
    Пусть $\rho_0, \rho_1$ --- положительные вероятностные плотности. Тогда
    \[
        \int \ln\frac{\rho_1(x)}{\rho_0(x)} \cdot \rho_1(x)dx \geq 0
    \]
    Равенство достигается тогда и только тогда, когда $\rho_0 = \rho_1$.
\end{lemma*}
\begin{proof}
    Домножим обе части неравенства на $(-1)$ и будем выводить оценку сверху:
    \[
        \int \ln\frac{\rho_0(x)}{\rho_1(x)} \cdot \rho_1(x)dx \overset{?}{\leq} 0
    \]
    Воспользуемся неравенством $\ln x \leq x - 1$ (очевидно, если, например, посмотреть на графики этих функций: у них есть единственное пересечение в
    точке $x = 1$):
    \[
        \ln\frac{\rho_0(x)}{\rho_1(x)}\rho_1(x) \leq \left( \frac{\rho_0}{\rho_1} - 1 \right)\rho_1 = \rho_0 - \rho_1
    \]
    Проинтегрируем обе части:
    \[
        \int \ln\frac{\rho_0(x)}{\rho_1(x)} \cdot \rho_1(x)dx \leq \int \rho_{0}(x)dx - \int \rho_{1}(x)dx = 0
    \]
    Оба интеграла справа равны 1, в силу того, что под интегралами стоят плотности. Таким образом оценку сверху мы доказали, найдём теперь, когда достигается равенство.

    Пусть в неравенстве достигается равенство, т.е. известно, что
    \begin{align*}
        &\int \ln\frac{\rho_0(x)}{\rho_1(x)} \cdot \rho_1(x)dx = 0
        &&\int \rho_{0}(x)dx - \int \rho_{1}(x)dx = 0 \iff \int \left(\frac{\rho_0}{\rho_1} - 1\right) \rho_1 dx = 0
    \end{align*}
    Тогда
    \[
        \int \left(\left(\frac{\rho_0}{\rho_1} - 1\right) - \ln\frac{\rho_0}{\rho_1}\right)\rho_{1}dx = 0
    \]
    Так как $\ln x \leq x - 1$, то $0 \leq x - 1 - \ln x$ и функция в скобках неотрицательна. Теперь очевидно, что 0 достигается только в случае
    $\rho_0 = \rho_1$:
    \[
        \left(\frac{\rho_0}{\rho_1} - 1\right) - \ln\frac{\rho_0}{\rho_1} = 0
        \iff
        \left(\frac{\rho_0}{\rho_1} - 1\right) = \ln\frac{\rho_0}{\rho_1}
        \iff
        \rho_0 = \rho_1
        \qedhere
    \]
\end{proof}

Вывели утверждение, которое показывает, что энтропия, в некотором смысле оценивает расстояние между плотностями, т.е. расстояние между распределениями.
Теперь будем применять это утверждение для построения оценки.

Пусть есть выборка $X = (X_1, \ldots, X_n)$ из распределения с обобщённой плотностью $\rho_{\theta}$. Пусть реальное значение параметра $\theta$
равно $\theta_1$. Рассмотрим функцию следующего вида:
\[
    W(\theta) = \EE_{\theta_1}\ln\rho_{\theta}(X_1) = \int \ln\rho_{\theta}(x)\rho_{\theta_1}(x)dx
\]
Можно показать, что $W(\theta) \leq W(\theta_1)\,\forall\,\theta$, действительно:
\begin{align*}
    W(\theta) - W(\theta_1) = \int \ln\rho_{\theta}(x)\rho_{\theta_1}(x)dx - \int \ln\rho_{\theta_1}(x)\rho_{\theta_1}(x)dx =
    \int \ln\frac{\rho_{\theta}(x)}{\rho_{\theta_1}(x)}\rho_{\theta_1}(x)dx \leq 0
\end{align*}
Причём наибольшее значение $W(\theta)$ достигается при $\theta = \theta_1$. Таким образом можно естественно оценить реальный параметр, если найти точку
максимума функции $W(\theta)$. В чём проблема: мы не знаем $\rho_{\theta_1}$, и потому функция $W(\theta)$ нам так же не известна. Решение проблемы:
$W(\theta)$ это некоторое матожидание. По ЗБЧ известно, что выборочное среднее по вероятности сходится к матожиданию. Т.е.
\[
    \frac{\ln\rho_{\theta}(X_1) + \ldots + \ln\rho_{\theta}(X_n)}{n} \xrightarrow{P} \EE_{\theta_1}\ln\rho_{\theta}(X_1) = W(\theta)
\]
Немного преобразуем левую часть:
\[
    \frac{\ln\rho_{\theta}(X_1) + \ldots + \ln\rho_{\theta}(X_n)}{n} = \frac{1}{n}\sum_{i = 1}^{n} \ln\rho_{\theta}(X_i) = \frac{1}{n}L(X, \theta)
\]
Таким образом, вместо того, чтобы искать максимум неизвестной функции, мы будем искаль максимум того, что к ней приближается, и найденное значение и будем
называть \textbf{оценкой максимального правдоподобия}.
\begin{definition}
    Оценкой максимального правдоподобия параметра $\theta$ называется максимум функции $L(X, \theta)$.
\end{definition}

\begin{proposal}
    Пусть $\theta \in(\alpha, \beta), \rho_{\theta}(x)$ непрерывна по $\theta$ и при каждом $x$ бункция $L(x, \theta)$ имеет ровно одну точку локального максимума $\theta_{n}(x)$ в $(\alpha, \beta) .$ Тогда $\theta_{n}(X)$ сходится по вероятности к $\theta_{0}$ (т.е. является состоятельной оценкой).
\end{proposal}
\begin{proof} \,
    
    Пусть $\delta>0$. Заметим, что $W\left(\theta_{0} \pm \delta\right)<W\left(\theta_{0}\right)$. Выберем $\alpha>0$ так, чтобы $W\left(\theta_{0}\right)-W\left(\theta_{0} \pm \delta\right)>2 \alpha$.
    
    Хотим оценить сверху $P_{\theta_{0}}(n^{-1}L(X,\theta_{0}\pm\delta)\geq n^{-1}L(X,\theta_{0}))$. Перейдём от множества (которое под знаком вероятности) к надмножеству из объединения нескольких удобных для анализа множеств:
%    $$
%    \begin{aligned}
%    A&\subseteq A\cap\{|n^{-1}L(X,\theta_{0}\pm\delta)-W(\theta_{0}\pm\delta)|\geq\alpha\}\\&\cup A\cap\{|n^{-1}L(X,\theta_{0})-W(\theta_{0})|\geq\alpha\}\\&\cup A\cap\{|n^{-1}L(X,\theta_{0}\pm\delta)-W(\theta_{0}\pm\delta)|<\alpha\}\cap\{|n^{-1}L(X,\theta_{0})-W(\theta_{0})|<\alpha\}\\A&\subseteq\{|n^{-1}L(X,\theta_{0}\pm\delta)-W(\theta_{0}\pm\delta)|\geq\alpha\}\\&\cup\{|n^{-1}L(X,\theta_{0})-W(\theta_{0})|\geq\alpha\}\\&\cup\{n^{-1}L(X,\theta_{0}\pm\delta)\geq n^{-1}L(X,\theta_{0}),\ |n^{-1}L(X,\theta_{0}\pm\delta)-W(\theta_{0}\pm\delta)|<\alpha,\ |n^{-1}L(X,\theta_{0})-W(\theta_{0})|<\alpha\}\\A&\subseteq\{|n^{-1}L(X,\theta_{0}\pm\delta)-W(\theta_{0}\pm\delta)|\geq\alpha\}\\&\cup\{|n^{-1}L(X,\theta_{0})-W(\theta_{0})|\geq\alpha\}\\&\cup\{W(\theta_{0}\pm\delta)+\alpha\ge n^{-1}L(X,\theta_{0}\pm\delta)\geq n^{-1}L(X,\theta_{0})\ge W(\theta_{0})-\alpha\}\\A&\subseteq\{|n^{-1}L(X,\theta_{0}\pm\delta)-W(\theta_{0}\pm\delta)|\geq\alpha\}\\&\cup\{|n^{-1}L(X,\theta_{0})-W(\theta_{0})|\geq\alpha\}\\&\cup\{W(\theta_{0}\pm\delta)+\alpha\ge W(\theta_{0})-\alpha\}.
%    \end{aligned}
%    $$
    \begingroup
    \allowdisplaybreaks
    \begin{align}
    A&:=\{n^{-1}L(X,\theta_{0}\pm\delta)\geq n^{-1}L(X,\theta_{0})\}\\B&:=\{|n^{-1}L(X,\theta_{0}\pm\delta)-W(\theta_{0}\pm\delta)|\geq\alpha\}\\C&:=\{|n^{-1}L(X,\theta_{0})-W(\theta_{0})|\geq\alpha\}\\D&:=\overline{B\cup C}=\overline{B}\cap\overline{C}\\&\\A&=A\cap(B\cup C\cup D)\\&=A\cap B\cup A\cap C\cup A\cap D\\&\subseteq B\cup C\cup A\cap D\\A\cap D&=\{n^{-1}L(X,\theta_{0}\pm\delta)\geq n^{-1}L(X,\theta_{0})\}\\&\quad\cap\{|n^{-1}L(X,\theta_{0}\pm\delta)-W(\theta_{0}\pm\delta)|<\alpha\}\\&\quad\cap\{|n^{-1}L(X,\theta_{0})-W(\theta_{0})|<\alpha\}\\&\subseteq\{W(\theta_{0}\pm\delta)+\alpha\ge n^{-1}L(X,\theta_{0}\pm\delta)\geq n^{-1}L(X,\theta_{0})\ge W(\theta_{0})-\alpha\}\\&\subseteq\{W(\theta_{0}\pm\delta)+\alpha\ge W(\theta_{0})-\alpha\}\\&\\A&\subseteq B\cup C\cup A\cap D\\&=\{|n^{-1}L(X,\theta_{0})-W(\theta_{0})|\geq\alpha\}\\&\quad\cup\{|n^{-1}L(X,\theta_{0}\pm\delta)-W(\theta_{0}\pm\delta)|\geq\alpha\}\\&\quad\cup\{W(\theta_{0}\pm\delta)+\alpha\ge W(\theta_{0})-\alpha\}.
    \end{align}
    \endgroup
    В итоге имеем
    $$
    \begin{aligned}
    P_{\theta_{0}}\left(n^{-1} L\left(X, \theta_{0} \pm \delta\right)\right.&\left.\geq n^{-1} L\left(X, \theta_{0}\right)\right) \leq P_{\theta_{0}}\left(\left|n^{-1} L\left(X, \theta_{0} \pm \delta\right)-W\left(\theta_{0} \pm \delta\right)\right| \geq \alpha\right) \\
    &+P_{\theta_{0}}\left(\left|n^{-1} L\left(X, \theta_{0}\right)-W\left(\theta_{0}\right)\right| \geq \alpha\right)+P_{\theta_{0}}\left(W\left(\theta_{0} \pm \delta\right)+\alpha \geq W\left(\theta_{0}\right)-\alpha\right),
    \end{aligned}
    $$
    причем первые две вероятности стремятся к нулю по ЗБЧ, т. к. $n^{-1} L\left(X, \theta\right) \xrightarrow{P_\theta} W\left(\theta\right)$, а третья вероятность равна нулю, т.к. $W\left(\theta_{0}\right)-W\left(\theta_{0} \pm \delta\right)>2 \alpha$. Тем самым,
    $$
    P\left(n^{-1} L\left(X, \theta_{0} \pm \delta\right)<n^{-1} L\left(X, \theta_{0}\right)\right) \rightarrow 1
    $$
    Посмотрим теперь на функцию $\theta \mapsto L(X, \theta)$:
    \begin{center}
    \begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=2.0cm,y=1.0cm]
        \draw[->,color=black] (-0.36,0) -- (5.33,0);
        \clip(-0.36,-1.35) rectangle (5.33,2.34);
        \begin{scriptsize}
        \fill [color=BLUE,shift={(0.51,0)},rotate=270] (0,0) ++(0 pt,3.75pt) -- ++(3.25pt,-5.625pt)--++(-6.5pt,0 pt) -- ++(3.25pt,5.625pt);
        \draw[color=BLUE] (0.5,-0.25) node {$\theta_0 - \delta$};
        \fill [color=BLUE,shift={(3.51,0)},rotate=90] (0,0) ++(0 pt,3.75pt) -- ++(3.25pt,-5.625pt)--++(-6.5pt,0 pt) -- ++(3.25pt,5.625pt);
        \draw[color=BLUE] (3.5,-0.25) node {$\theta_0 + \delta$};
        \fill [color=BLACK] (2.01,0) circle (1.5pt);
        \draw[color=BLACK] (2.05,-0.25) node {$\theta_0$};
        \fill [color=BLACK] (0.51,1.04) circle (1.5pt);
        \fill [color=BLACK] (3.51,0.54) circle (1.5pt);
        \fill [color=BLACK] (2.01,2.04) circle (1.5pt);
        \end{scriptsize}
    \end{tikzpicture}
    \end{center}
    Если $L\left(x, \theta_{0} \pm \delta\right)<L\left(x, \theta_{0}\right)$, то на интервале $\left(\theta_{0}-\delta, \theta_{0}+\delta\right)$ есть точка локального максимума. По предположению теоремы это означает, что $\theta_{n}(x) \in\left[\theta_{0}-\delta, \theta_{0}+\delta\right]$. Таким образом, имеем
    $$
    \lim _{n \rightarrow \infty} P\left(\left|\theta_{n}(X)-\theta_{0}\right| \leq \delta\right)=1
    $$
    Теорема доказана.
\end{proof}

