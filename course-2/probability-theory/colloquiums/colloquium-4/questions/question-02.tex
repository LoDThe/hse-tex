\section{Билет 2}

\begin{center}
    \it
    Метод моментов и его состоятельность.
    Метод максимального правдоподобия.
    Энтропия и состоятельность оценки максимального правдоподобия.
\end{center}

\usetikzlibrary{arrows}
\definecolor{BLACK}{rgb}{0,0,1}
\definecolor{BLUE}{rgb}{0.2,0.2,1}

\subsection{Метод моментов и его состоятельность}
Пусть $(X_1, \ldots, X_n)$ --- выборка, где $X_i$ задано распределением $F_{\theta}$. Хотим найти состоятельную оценку параметра $\theta$.
Пусть $g$ --- непрерывная функция, причём $\EE_{\theta}\left( |g(X_1)| \right) < \infty$. Посчитаем матожидание
$\EE_{\theta}\left( g(X_1) \right) = f(\theta)$. Предположим, что $\exists\, f^{-1}\,$, и она непрерывна. Т.к. на практике мы не знаем параметр
$\theta$, то мы не можем посчитать такое матожидание. Но мы можем приближённо посчитать $f(\theta)$ воспользовавшись ЗБЧ.

По ЗБЧ
\[
    \frac{g(X_1) + \ldots + g(X_n)}{n} \xrightarrow{P} \EE_{\theta}\left( g(X_1) \right) = f(\theta)
\]
Теперь в силу обратимости $f$ можно получить сходимость к $\theta$:
\[
    f^{-1}\left( \frac{g(X_1) + \ldots + g(X_n)}{n} \right) \xrightarrow{P} \theta
\]
Оценкой параметра $\theta$ назовём функцию $\hat{\theta}(X_1, \ldots, X_n) = f^{-1}\left( \frac{g(X_1) + \ldots + g(X_n)}{n} \right)$. Состоятельность
оценки очевидна. Вместо ЗБЧ можно применять УЗБЧ, и получить сильную состоятельность.

\subsection{Метод максимального правдоподобия}
\subsection{Энтропия и состоятельность оценки максимального правдоподобия}
\begin{definition}
    Обобщённой плотностью $\rho_X$ случайной величины $X$ назовём функцию плотности $X$, если случайная величина является непрерывно, или функцию
    $\rho_X(t) = P(X = t)$ в случае, если $X$ имеет дискретное распределение.
\end{definition}
\begin{definition}
    Пусть $X = (X_1, \ldots, X_n)$ --- выборка из распределения с обобщённой плотностью $\rho_{\theta}$. Обобщённая плотность вектора $X$ называется функцией
    правдоподобия, и имеет вид
    \[
        p(X, \theta) = \rho_{\theta}(X_1) \cdot \ldots \cdot \rho_{\theta}(X_n)
    \]
    Функцию $\ln p(X, \theta)$ называют логарифмической функцией правдоподобия и обозначают $L(X, \theta)$.
\end{definition}
\begin{definition}
    Пусть $\rho_0, \rho_1$ --- положительные вероятностные плотности. Выражение
    \[
        \int \ln\frac{\rho_1(x)}{\rho_0(x)} \cdot \rho_1(x)dx
    \]
    называется энтропией распределения с плотностью $\rho_1$ относительно распределения с плотностью $\rho_0$.
\end{definition}
\begin{comment}
    Здесь и далее интергралы без пределов интегрирования обозначают интегрирование по множеству, на котором задано распределение. Они вовсе не означают
    неопределённый интеграл.
\end{comment}

Следующее утверждение показывает, что энтропия в некотором смысле оценивает расстояние между распределениями:
\begin{lemma*} (Информационное неравенство)

    Пусть $\rho_0, \rho_1$ --- положительные вероятностные плотности. Тогда
    \[
        \int \ln\frac{\rho_1(x)}{\rho_0(x)} \cdot \rho_1(x)dx \geq 0
    \]
    Равенство достигается тогда и только тогда, когда $\rho_0 = \rho_1$.
\end{lemma*}
\begin{proof}
    Домножим обе части неравенства на $(-1)$ и будем выводить оценку сверху:
    \[
        \int \ln\frac{\rho_0(x)}{\rho_1(x)} \cdot \rho_1(x)dx \overset{?}{\leq} 0
    \]
    Воспользуемся неравенством $\ln x \leq x - 1$ (очевидно, если, например, посмотреть на графики этих функций: у них есть единственное пересечение в
    точке $x = 1$):
    \[
        \ln\frac{\rho_0(x)}{\rho_1(x)}\rho_1(x) \leq \left( \frac{\rho_0}{\rho_1} - 1 \right)\rho_1 = \rho_0 - \rho_1
    \]
    Проинтегрируем обе части:
    \[
        \int \ln\frac{\rho_0(x)}{\rho_1(x)} \cdot \rho_1(x)dx \leq \int \rho_{0}(x)dx - \int \rho_{1}(x)dx
    \]
    Оба интеграла слева равны 1, в силу того, что под интегралами стоят плотности. Таким образом оценку сверху мы доказали, найдём теперь,
    когда достигается равенство.

    Пусть в неравенстве достиглось равенство, т.е. известно, что
    \begin{align*}
        &\int \ln\frac{\rho_0(x)}{\rho_1(x)} \cdot \rho_1(x)dx = 0
        &&\int \rho_{0}(x)dx - \int \rho_{1}(x)dx = 0 \iff \int \left(\frac{\rho_0}{\rho_1} - 1\right)dx = 0
    \end{align*}
    Тогда
    \[
        \int \left(\left(\frac{\rho_0}{\rho_1} - 1\right) - \ln\frac{\rho_0}{\rho_1}\right)\rho_{1}dx = 0
    \]
    Так как $\ln x \leq x - 1$, то $0 \leq x - 1 - \ln x$, и функция в скобках неорицательна. Теперь очевидно, что 0 достигается только в случае
    $\rho_0 = \rho_1$:
    \[
        \left(\frac{\rho_0}{\rho_1} - 1\right) - \ln\frac{\rho_0}{\rho_1} = 0
        \iff
        \left(\frac{\rho_0}{\rho_1} - 1\right) = \ln\frac{\rho_0}{\rho_1}
        \iff
        \rho_0 = \rho_1
    \]
\end{proof}

Вывели утверждение, которое показывает, что энтропия, в некотором смысле оценивает расстояние между плотностями, т.е. расстояние между распределениями.
Теперь будем применять это утверждение для построения оценки.

Пусть есть выборка $X = (X_1, \ldots, X_n)$ из распределения с обобщённой плотностью $\rho_{\theta}$. Пусть реальное значение параметра $\theta$
равно $\theta_1$. Рассмотрим функцию следующего вида:
\[
    W(\theta) = \EE_{\theta_1}\ln\rho_{\theta}(X_1) = \int \ln\rho_{\theta}(x)\rho_{\theta_1}(x)dx
\]
Можно показать, что $W(\theta) \leq W(\theta_1)\,\forall\,\theta$, действительно:
\begin{align*}
    W(\theta) - W(\theta_1) = \int \ln\rho_{\theta}(x)\rho_{\theta_1}(x)dx - \int \ln\rho_{\theta_1}(x)\rho_{\theta_1}(x)dx =
    \int \ln\frac{\rho_{\theta}(x)}{\rho_{\theta_1}(x)}\rho_{\theta_1}(x)dx \leq 0
\end{align*}
Причём наибольшее значение $W(\theta)$ достигается при $\theta = \theta_1$. Таким образом можно естественно оценить реальный параметр, если найти точку
максимума функции $W(\theta)$. В чём проблема: мы не знаем $\rho_{\theta_1}$, и потому функция $W(\theta)$ нам так же не известна. Решение проблемы:
$W(\theta)$ это некоторое матожидание. По ЗБЧ известно, что выборочное среднее по вероятности сходится к матожиданию. Т.е.
\[
    \frac{\ln\rho_{\theta}(X_1) + \ldots + \ln\rho_{\theta}(X_n)}{n} \xrightarrow{P} \EE_{\theta_1}\ln\rho_{\theta}(X_1) = W(\theta)
\]
Немного преобразуем левую часть:
\[
    \frac{\ln\rho_{\theta}(X_1) + \ldots + \ln\rho_{\theta}(X_n)}{n} = \frac{1}{n}\sum_{i = 1}^{n} \ln\rho_{\theta}(X_i) = \frac{1}{n}L(X, \theta)
\]
Таким образом, вместо того, чтобы искать максимум неизвестной функции, мы будем искаль максимум того, что к ней приближается, и найденное значение и будем
называть \textbf{оценкой максимального правдоподобия}.
\begin{definition}
    Оценкой максимального правдоподобия параметра $\theta$ называется максимум функции $L(X, \theta)$.
\end{definition}
\begin{proposal} (Состоятельность оценки максимального правдоподобия.)

    Пусть $\theta \in (a, b)$, и на этом отрезке функция $\theta \to L(X, \theta)$ имеет единственную точку локального максимума $\hat{\theta}$.
    Тогда $\hat{\theta} \xrightarrow{P} \theta_0$.
\end{proposal}
\begin{proof}
    Будем пользоваться тем, что $\frac{1}{n}L(X, \theta) \xrightarrow{P} W(\theta)$. Хотим доказать, что $P(|\hat{\theta} - \theta_0| \geq \delta) \to 0\,
    \forall\,\delta > 0$ (просто определение сходимости по вероятности). Рассмотрим точки $\theta_0 - \delta, \theta_0 + \delta$. Про эти точки известно
    следующее:
    \[
        \begin{cases}
            W(\theta_0) > W(\theta_0 - \delta) \xleftarrow{P} \cfrac{1}{n}L(X, \theta_0 - \delta)\\
            W(\theta_0) > W(\theta_0 + \delta) \xleftarrow{P} \cfrac{1}{n}L(X, \theta_0 + \delta)\\
        \end{cases}
    \]
    Можно ожидать, что при достаточно большом $n$, с вероятностью, близкой к $1$ будут выполнены неравенства
    \[
        \begin{cases}
            \cfrac{1}{n}L(X, \theta_0) > \cfrac{1}{n}L(X, \theta_0 - \delta)\\
            \cfrac{1}{n}L(X, \theta_0) > \cfrac{1}{n}L(X, \theta_0 + \delta)\\
        \end{cases}
    \]
    Посмотрим теперь на функцию $\theta \to L(X, \theta)$:
    \[
        \begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=2.0cm,y=1.0cm]
            \draw[->,color=black] (-0.36,0) -- (5.33,0);
            \clip(-0.36,-1.35) rectangle (5.33,2.34);
            \begin{scriptsize}
                \fill [color=BLUE,shift={(0.51,0)},rotate=270] (0,0) ++(0 pt,3.75pt) -- ++(3.25pt,-5.625pt)--++(-6.5pt,0 pt) -- ++(3.25pt,5.625pt);
                \draw[color=BLUE] (0.5,-0.25) node {$\theta_0 - \delta$};
                \fill [color=BLUE,shift={(3.51,0)},rotate=90] (0,0) ++(0 pt,3.75pt) -- ++(3.25pt,-5.625pt)--++(-6.5pt,0 pt) -- ++(3.25pt,5.625pt);
                \draw[color=BLUE] (3.5,-0.25) node {$\theta_0 + \delta$};
                \fill [color=BLACK] (2.01,0) circle (1.5pt);
                \draw[color=BLACK] (2.05,-0.25) node {$\theta_0$};
                \fill [color=BLACK] (0.51,1.04) circle (1.5pt);
                \fill [color=BLACK] (3.51,0.54) circle (1.5pt);
                \fill [color=BLACK] (2.01,2.04) circle (1.5pt);
            \end{scriptsize}
        \end{tikzpicture}
    \]
    Ясно, что на интервале $(\theta_0 - \delta, \theta_0 + \delta)$ сущетвует точка, значение в которой строго больше, чем на концах, а значит,
    функция имеет на этом отрезке точку локального максимума. Т.е. для точки локального максимума $\hat{\theta}$ выполнено $|\hat{\theta} - \theta_0| < \delta$.
    Чтобы завершить доказательство, нужно обосновать фразу ``при достаточно большом $n$, с вероятностью, близкой к $1$...''.
    Другими словами, хотим доказать, что
    \[
        P\left( \frac{1}{n} L(X, \theta_0 - \delta) < \frac{1}{n} L(X, \theta_0) \right) \xrightarrow[n \to \infty]{} 1
    \]
    Положим $W(\theta_0) - W(\theta_0 - \delta) = \epsilon > 0$. Из ЗБЧ следует, что
    \[
        \begin{cases}
            P\left(\left| \frac{1}{n} L(X, \theta_0 - \delta) - W(\theta_0 - \delta) \right| \geq \frac{\epsilon}{4} \right) \to 0\\
            P\left( \left| \frac{1}{n} L(X, \theta_0) - W(\theta_0) \right| \geq \frac{\epsilon}{4} \right) \to 0
        \end{cases}
    \]
    Поймём, почему из этого следует, что $P\left(\frac{1}{n} L(X, \theta_0 - \delta) \geq  \frac{1}{n} L(X, \theta_0)\right) \to 0 \, (*)$.
    Пусть величины $\frac{1}{n} L(X, \theta_0 - \delta)$ и $W(\theta_0 - \delta)$ отличаются менее, чем на $\frac{\epsilon}{4}$. Аналогично для
    $\frac{1}{n} L(X, \theta_0)$ и $W(\theta_0)$. Тогда верна следующая цепочка равенств:
    \[
        W(\theta_0) \leq \frac{1}{n}L_n(X, \theta_0) + \frac{\epsilon}{4} \overset{(1)}{\leq} \frac{1}{n}L_n(X, \theta_0 - \delta) + \frac{\epsilon}{4}
        \leq W(\theta_0 - \delta) + \frac{\epsilon}{2}
    \]
    Переход $(1)$ следует из неравенства $(*)$. Тогда мы получаем, что $W(\theta_0) - W(\theta_0 - \delta) \leq \frac{\epsilon}{2}$. Но
    $W(\theta_0) - W(\theta_0 - \delta) = \epsilon$, и получается противоречие. Значит, или
    $\frac{1}{n} L(X, \theta_0 - \delta)$ и $W(\theta_0 - \delta)$ отличаются более, чем на $\frac{\epsilon}{4}$, или же величины
    $\frac{1}{n} L(X, \theta_0)$ и $W(\theta_0)$. Но тогда мы получаем, что исход из события
    $\{\frac{1}{n} L(X, \theta_0 - \delta) \geq  \frac{1}{n} L(X, \theta_0)\}$ лежит в объединении
    \[
        \{\left| \frac{1}{n} L(X, \theta_0 - \delta) - W(\theta_0 - \delta) \right| \geq \frac{\epsilon}{4}\}
        \cup
        \{\left| \frac{1}{n} L(X, \theta_0) - W(\theta_0) \right| \geq \frac{\epsilon}{4}\}
    \]
    А вероястность таких событий стремится к нулю. Теперь методом пристального взгляда можно заметить, что мы всё доказали.
\end{proof}
